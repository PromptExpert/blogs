Comments:

- The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought.[^1]
- o1 is a model that thinks before giving the final answer. In my own words, here are the biggest updates to the field of AI.[^2]
- Donâ€™t do chain of thought purely via prompting, train models to do better chain of thought using RL.[^2]
- In the history of deep learning we have always tried to scale training compute, but chain of thought is a form of adaptive compute that can also be scaled at inference time. [^2]
- Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. [^3]
- Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isnâ€™t working. This process dramatically improves the modelâ€™s ability to reason.[^3]
- We're no longer limited by pretraining paradigm; now, we can scale through inference compute, opening up new possibilities for capabilities and alignment.[^4]
- The interesting update from Strawberry is that OpenAI has found a way to add a new dimension on which to improve performance: compute during inference. The company has found that when Strawberry takes longer to respond to a promptâ€”in other words, when itâ€™s given more time to thinkâ€”it generally responds more accurately.[^5]
- OpenAI released their new reasoning system, o1, building on the early successes of Q* and more recently the rumors of Strawberry, to ship a new mode of interacting with AI on challenging tasks. o1 is a system designed by training new models on long reasoning chains, with lots of reinforcement learning ğŸ’, and deploying them at scale. Unlike traditional autoregressive language models, it is doing an online search for the user. It is spending more on inference, which confirms the existence of new scaling laws â€” inference scaling laws.[^6]
- When referring to o1, it is best to refer to it as a system. Thereâ€™s a chance all the operations are funneled through one advanced language model, but the funneling and recycling of those computations in a way that creates coherent outputs for the user is very complex. [^6]
- OpenAI Strawberry (o1) is out! We are finally seeing the paradigm of inference-time scaling popularized and deployed in production. As Sutton said in the Bitter Lesson, there're only 2 techniques that scale indefinitely with compute: learning & search. It's time to shift focus to the latter. [^7]
- You don't need a huge model to perform reasoning. Lots of parameters are dedicated to memorizing facts, in order to perform well in benchmarks like trivia QA. It is possible to factor out reasoning from knowledge, i.e. a small "reasoning core" that knows how to call tools like browser and code verifier. Pre-training compute may be decreased.[^7]
- A huge amount of compute is shifted to serving inference instead of pre/post-training. LLMs are text-based simulators. By rolling out many possible strategies and scenarios in the simulator, the model will eventually converge to good solutions. The process is a well-studied problem like AlphaGo's monte carlo tree search (MCTS).[^7]



Reading List:

- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
- https://github.com/hijkzzz/Awesome-LLM-Strawberry
- [A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/abs/2408.01072) 
- [Reverse Engineering o1 Architecture](https://www.reddit.com/r/LocalLLaMA/comments/1fgr244/reverse_engineering_o1_architecture_with_a_little/)
- [Reverse engineering OpenAIâ€™s o1](https://www.interconnects.ai/p/reverse-engineering-openai-o1)
- [Improving LLM Reasoning using SElf-generated data:RL and Verifiers](https://drive.google.com/file/d/1komQ7s9kPPvDx_8AxTh9A6tlfJA0j6dR/view)
- [o1 å‘å¸ƒåï¼Œä¿¡æ¯é‡æœ€å¤§çš„åœ†æ¡Œå¯¹è¯ï¼šæ¨æ¤éºŸã€å§œå¤§æ˜•ã€æœ±å†›æ¢è®¨å¤§æ¨¡å‹æŠ€æœ¯è·¯å¾„](https://mp.weixin.qq.com/s/FSiCYyc1W6CFCT_eCwVSsw)
- [Building OpenAI o1 (Extended Cut)](https://www.youtube.com/watch?v=tEzs3VHyBDM)

References:

[^1]: OpenAI o1 System Card </br>
[^2]: Jason Wei </br> 
[^3]: https://openai.com/index/learning-to-reason-with-llms/ </br>
[^4]: Mira Murati </br>
[^5]: https://every.to/chain-of-thought/openai-s-o1-model-explained </br>
[^6]: https://www.interconnects.ai/p/reverse-engineering-openai-o1 </br>
[^7]: Jim Fan </br>
