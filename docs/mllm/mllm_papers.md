# MLLM论文精选（持续更新）

多模态交流QQ群: 237976286

## 最新动态
- 2025.01 [Qwen2.5 VL](https://qwenlm.github.io/blog/qwen2.5-vl/)
- 2025.01 [VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/abs/2501.13106)
- 2025.01 [vikhyatk/moondream2](https://hf-mirror.com/vikhyatk/moondream2) 
- 2025.01 [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/pdf/2501.05874)
- 2025.01 [MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o) MiniCPM-V升级版。
- 2025.01 [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/abs/2501.07542)
- 2025.01 [Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos](https://github.com/magic-research/Sa2VA)
- 2025.01 [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958) 达摩院开源的多模态数据集，由22,000小时的上课视频而来。
- 2024.12 [MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/abs/2412.14164v1) 
- 2024.12 [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://apollo-lmms.github.io/) Meta出品的Video-LLM
- 2024.12 [DeepSeek-VL2](https://github.com/deepseek-ai/DeepSeek-VL2/tree/main)
- 2024.12 [FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/html/2412.13303v1)
- 2024.12 [POINTS1.5 Buiding a Vision-Language Model towards Real World Applications](https://github.com/WePOINTS/WePOINTS/blob/main/POINTS1_5_preview.pdf) 微信出品。
- 2024.12 [InternVL 2.5](https://hf-mirror.com/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c)  1B 到 78B 都有。
- 2024.12 [Qwen2-VL-72B Model](https://hf-mirror.com/Qwen/Qwen2-VL-72B) 
- 2024.12 [Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://arxiv.org/abs/2412.04424) 
- 2024.12 [NVILA: Efficient Frontier Visual Language Models](https://arxiv.org/abs/2412.04468) NVIDIA出品，同时优化效率和准确率的VLM。
- 2024.12 [PaliGemma 2:A Family of Versatile VLMs for Transfer](https://arxiv.org/pdf/2412.03555) 
- 2024.11 [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/pdf/2411.14402) 苹果提出全新的视觉编码器训练方式，支持多模态。
- 2024.11 [Pixtral Large](https://mistral.ai/news/pixtral-large/) Mistral发布124B的多模态大模型。
- 2024.11 [OmniVision-968M: World's Smallest Vision Language Model](https://nexa.ai/blogs/omni-vision)
- 2024.11 [LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation](https://microsoft.github.io/LLM2CLIP/) 微软出品，将CLIP中的text encoder替换成LLM，支持更长的上下文和更复杂的文本，有更好的topk检索效果。
- 2024.11 [HourVideo: 1-Hour Video-Language Understanding](https://arxiv.org/abs/2411.04998) 李飞飞团队提出长视频理解评测集
- 2024.11 [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996)
- 2024.11 [MM-EMBED: UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS](https://arxiv.org/pdf/2411.02571) 英伟达提出基于MLLM的通用多模态检索。
- 2024.11 [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391) 
- 2024.11 [Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework](https://arxiv.org/abs/2411.01639) 提高多模态基础模型在处理不确定性时的能力，从而增强机器人在规划任务中的可靠性。
- 2024.10 [Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities](https://arxiv.org/abs/2410.11190)
- 2024.10 [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434) Meta提出长视频理解方法。
- 2024.10 [Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data](https://arxiv.org/pdf/2410.18558) 智源开源4千万多模态指令数据。
- 2024.10 [Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance](https://arxiv.org/abs/2410.16261) 
- 2024.10 [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/pdf/2409.04429) VILA团队的统一理解和生成模型。
- 2024.10 [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2410.13848) DeepSeek首个多模态模型。
- 2024.10 [ARIA : An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/pdf/2410.05993) 3.9B模型，号称超过 Pixtral-12B 和 Llama3.2-11。
- 2024.10 [BAICHUAN-OMNI TECHNICAL REPORT](https://arxiv.org/pdf/2410.08565) 百川首个7B多模态模型。
- 2024.10 [Pixtral 12B](https://arxiv.org/abs/2410.07073) Mistral出品。
- 2024.10 [Movie Gen: A Cast of Media Foundation Models](https://ai.meta.com/static-resource/movie-gen-research-paper) Meta出品
- 2024.10 [LEOPARD : A Vision Language Model for Text-Rich Multi-Image Tasks](https://arxiv.org/pdf/2410.01744) 
- 2024.10 [Video Instruction Tuning with Synthetic Data](https://llava-vl.github.io/blog/2024-09-30-llava-video/) LLaVA和字节合作开源视频指令数据
- 2024.09 [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) 苹果MM1的升级版。
- 2024.09 [Emu3: Next-Token Prediction is All You Need](https://emu.baai.ac.cn/about) BAAI出品。
- 2024.09 [Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Multimodal Models](https://www.arxiv.org/abs/2409.17146) Allen出品，同时开源模型和数据。
- 2024.09 [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692)
- 2024.09 [Phantom of Latent for Large Language and Vision Models](https://arxiv.org/abs/2409.14713)
- 2024.09 [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191)
- 2024.09 [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) 
- 2024.09 [NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/pdf/2409.11402) 英伟达出品。
- 2024.09 [Viper: Open Mamba-based Vision-Language Models](https://github.com/EvanZhuang/viper/tree/main) 首个基于Mamba的VLM系列
- 2024.09 [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/abs/2407.21770)
- 2024.09 [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/pdf/2409.01704v1) 
- 2024.09 [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://www.arxiv.org/pdf/2409.04429) 
- 2024.08 [Law of Vision Representation in MLLMs](https://arxiv.org/abs/2408.16357) 提出了AC score指标，AC score越高，视觉表示越好。
- 2024.08 [CogVLM2: Visual Language Models for Image and Video Understanding](https://arxiv.org/abs/2408.16500)
- 2024.08 [EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders](https://arxiv.org/pdf/2408.15998)
- 2024.08 [A Practitioner's Guide to Continual Multimodal Pretraining](https://www.arxiv.org/abs/2408.14471)
- 2024.08 [Building and better understanding vision-language models: insights and future directions](https://www.arxiv.org/pdf/2408.12637)
- 2024.08 [LongVILA: Scaling Long-Context Visual Language Models for Long Videos](https://arxiv.org/abs/2408.10188)
- 2024.08 [UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling](https://arxiv.org/pdf/2408.04810)
- 2024.08 [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://www.arxiv.org/abs/2408.08872) 
- 2024.08 [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) LLaVA-NeXT系列的集大成。
- 2024.08 [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800) 超强的小钢炮MLLM。
- 2024.08 [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714) 

## 经典论文
- 2021.02 [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020) CLIP
- 2022.04 [Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf) DeepMind出品，MLLM先驱。
- 2023.01 [BLIP-2](https://arxiv.org/abs/2301.12597) 提出Q-Former。
- 2023.03 [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) CLIP的变种替代品，Sigmoid损失。
- 2023.04 [MiniGPT-4](https://arxiv.org/abs/2304.10592) 热度很高。
- 2023.04 [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) LLaVA系列的第一篇文章。
- 2023.05 [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) 
- 2023.05 [Segment Anything](https://arxiv.org/abs/2304.02643) SAM
- 2023.12 [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805)
- 2024.01 [AGENT AI:SURVEYING THE HORIZONS OF MULTIMODAL INTERACTION](https://arxiv.org/pdf/2401.03568) 李飞飞团队出品。
- 2024.04 [MM1- Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) 苹果出品。
- 2024.05 [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247) Meta出品，短小精悍。
- 2024.05 [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/pdf/2403.05525)
- 2024.06 [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860) 提出以视觉为中心的benchmark CV-Bench，实验探究各个方面对VLM表现的影响，训练Cambrian-1模型。
- 2024.09 [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- 2024.09 [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191)
- 2024.09 [Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Multimodal Models](https://www.arxiv.org/abs/2409.17146) Allen出品，同时开源模型和数据。
